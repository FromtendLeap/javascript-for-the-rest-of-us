# FASE 3: Sistema Completo de Detecci√≥n Facial

## Objetivo de la fase
Integrar TensorFlow.js para detecci√≥n facial y completar el sistema autom√°tico que pausa/reproduce el video seg√∫n la atenci√≥n del usuario.

## Timing estimado: 55 minutos

---

### Concepto 1: Carga de modelos de IA
- **Explicaci√≥n**: TensorFlow.js nos permite cargar modelos pre-entrenados como BlazeFace. Es como descargar un "cerebro especializado" que ya sabe detectar caras.
- **Demo RunJS**:
```js
// Simular la carga de un modelo de IA
async function simularCargaModelo() {
  console.log('üß† Iniciando carga del modelo...');
  
  // Simular tiempo de descarga
  await new Promise(resolve => setTimeout(resolve, 2000));
  
  console.log('‚úÖ Modelo cargado y listo para usar');
  
  return {
    name: 'BlazeFace',
    loaded: true,
    detect: function() { return 'detectando...'; }
  };
}

const modelo = await simularCargaModelo();
console.log('Modelo disponible:', modelo.name);
```
- **Aplicaci√≥n a la fase**: Creamos la funci√≥n `loadModel()` del c√≥digo original
```js
async function loadModel() {
  const model = await blazeface.load();
  return model;
}
```
- **Resultado esperado**: Tenemos la funci√≥n que carga el modelo BlazeFace

---

### Concepto 2: setInterval para detecci√≥n continua
- **Explicaci√≥n**: Para saber si el usuario est√° atento, necesitamos verificar continuamente. `setInterval` ejecuta una funci√≥n cada X milisegundos autom√°ticamente.
- **Demo RunJS**:
```js
let verificaciones = 0;

function verificarEstado() {
  verificaciones++;
  const timestamp = new Date().toLocaleTimeString();
  console.log(`Verificaci√≥n #${verificaciones} a las ${timestamp}`);
  
  if (verificaciones >= 5) {
    clearInterval(intervalId);
    console.log('üõë Detecci√≥n detenida');
  }
}

console.log('üé¨ Iniciando detecci√≥n cada 1.5 segundos...');
const intervalId = setInterval(verificarEstado, 1500);
```
- **Aplicaci√≥n a la fase**: Creamos las funciones `startDetecting()` y `stopDetecting()`
```js
async function startDetecting() {
  await initCamera();
  model = await loadModel();
  
  // Sistema de detecci√≥n b√°sico (iremos completando paso a paso)
  checkInterval = setInterval(async () => {
    console.log('üîç Verificando atenci√≥n cada 2 segundos...');
    // Aqu√≠ a√±adiremos detectFaces(), visualizeFaces() y handleAttentionState()
    // en los conceptos siguientes
  }, CHECK_INTERVAL_RANGE);
}

function stopDetecting() {
  clearInterval(checkInterval);
  console.log('üõë Detecci√≥n detenida');
}
```
- **Resultado esperado**: Tenemos las funciones que controlan la detecci√≥n autom√°tica

---

### Concepto 3: Detecci√≥n facial + Arrays
- **Explicaci√≥n**: Usamos el modelo para analizar la webcam, pero primero necesitamos entender **arrays** porque `detectFaces()` devuelve una "lista de caras detectadas". Un array es como una "lista de la compra" - puede tener varios elementos en orden.
- **Demo RunJS**:
```js
// Primero entendemos arrays
const frutasVacio = [];
const frutas = ['manzana', 'banana', 'naranja'];

console.log('Frutas:', frutas);
console.log('¬øCu√°ntas frutas?', frutas.length);

// Comprobar si hay elementos
if (frutas.length > 0) {
  console.log('Tenemos frutas disponibles');
} else {
  console.log('No hay frutas');
}

// Recorrer todos los elementos
frutas.forEach((fruta, index) => {
  console.log(`Fruta ${index + 1}: ${fruta}`);
});

// Ahora simulamos detecci√≥n facial
const carasSimuladas = [
  { topLeft: [50, 60], bottomRight: [150, 180] },
  { topLeft: [200, 80], bottomRight: [280, 160] }
];

console.log('Caras detectadas:', carasSimuladas.length);
console.log('¬øHay caras?', carasSimuladas.length > 0);
```
- **Aplicaci√≥n a la fase**: Creamos `detectFaces()` que devuelve un array + lo integramos
```js
async function detectFaces() {
  if (!model) throw TypeError('Model not loaded');

  const predictions = await model.estimateFaces(webcamVideo, false);

  return predictions; // ¬°Devuelve un array de caras detectadas!
}

// Actualizamos startDetecting() para usar detectFaces()
async function startDetecting() {
  await initCamera();
  model = await loadModel();
  
  checkInterval = setInterval(async () => {
    console.log('üîç Detectando caras...');
    const faces = await detectFaces(); // ¬°Array de caras!
    console.log('Caras encontradas:', faces.length); // Usar .length del array
    console.log('¬øHay caras?', faces.length > 0); // Comprobar si array tiene elementos
    // Seguiremos a√±adiendo m√°s funciones paso a paso
  }, CHECK_INTERVAL_RANGE);
}
```
- **Resultado esperado**: El sistema detecta caras (array) y muestra informaci√≥n sobre ellas

---

### Concepto 4: Control autom√°tico del video seg√∫n atenci√≥n
- **Explicaci√≥n**: Usamos el modelo para analizar el frame actual de la webcam. Devuelve un array con informaci√≥n sobre las caras detectadas.
- **Demo RunJS**:
```js
// Simular el proceso de detecci√≥n facial
async function simularDeteccionFacial() {
  console.log('üîç Analizando frame de video...');
  
  // Simular tiempo de procesamiento
  await new Promise(resolve => setTimeout(resolve, 100));
  
  // Simular resultados variables
  const scenarios = [
    [], // Sin caras
    [{ // Una cara detectada
      topLeft: [50, 60], 
      bottomRight: [150, 180]
    }]
  ];
  
  const result = scenarios[Math.floor(Math.random() * scenarios.length)];
  console.log(`üìä Resultado: ${result.length} cara(s) detectada(s)`);
  
  return result;
}

const caras = await simularDeteccionFacial();
console.log('Caras:', caras);
```
- **Aplicaci√≥n a la fase**: Creamos `detectFaces()` y la a√±adimos al sistema
```js
async function detectFaces() {
  if (!model) throw TypeError('Model not loaded');

  const predictions = await model.estimateFaces(webcamVideo, false);

  return predictions; // ¬°Devuelve un array de caras detectadas!
}

// Ahora actualizamos startDetecting() para usar detectFaces()
async function startDetecting() {
  await initCamera();
  model = await loadModel();
  
  checkInterval = setInterval(async () => {
    console.log('üîç Detectando caras...');
    const faces = await detectFaces(); // ¬°Ahora s√≠ existe!
    console.log('Caras encontradas:', faces.length);
    // Seguiremos a√±adiendo m√°s funciones paso a paso
  }, CHECK_INTERVAL_RANGE);
}
```
- **Resultado esperado**: El sistema detecta caras y muestra cu√°ntas encuentra

---

### Concepto 5: Control autom√°tico del video seg√∫n atenci√≥n
- **Explicaci√≥n**: Seg√∫n si detectamos caras o no, decidimos autom√°ticamente si pausar o reproducir el video. Tambi√©n actualizamos la interfaz.
- **Demo RunJS**:
```js
// Simular el control autom√°tico del video
const mockVideo = {
  paused: false,
  play: () => console.log('‚ñ∂Ô∏è Video reproducido autom√°ticamente'),
  pause: () => console.log('‚è∏Ô∏è Video pausado autom√°ticamente')
};

function simularControlAutomatico(hayCaras) {
  console.log(`üéØ Procesando: ${hayCaras ? 'CARA DETECTADA' : 'SIN CARA'}`);
  
  if (hayCaras) {
    console.log('üü¢ Usuario atento - reproducir video');
    mockVideo.play();
  } else {
    console.log('üî¥ Usuario no atento - pausar video');
    mockVideo.pause();
  }
}

// Simular diferentes escenarios
simularControlAutomatico(true);
setTimeout(() => simularControlAutomatico(false), 2000);
```
- **Aplicaci√≥n a la fase**: Creamos `handleAttentionState()` y la a√±adimos al sistema
```js
function handleAttentionState(areFacesDetected) {
  if (areFacesDetected) {
    attentionStatusElement.textContent = '‚úÖ Atento';
    attentionStatusElement.className = 'paying-attention';
    document.body.classList.remove('not-paying-attention');

    return videoPlayer.play();
  }

  attentionStatusElement.textContent = '‚ùå No atento';
  attentionStatusElement.className = 'not-paying-attention';
  document.body.classList.add('not-paying-attention');

  videoPlayer.pause();
}

// Actualizamos startDetecting() para usar handleAttentionState()
async function startDetecting() {
  await initCamera();
  model = await loadModel();
  
  checkInterval = setInterval(async () => {
    const faces = await detectFaces();
    console.log('Caras encontradas:', faces.length);
    handleAttentionState(faces.length > 0); // ¬°Control autom√°tico del video!
    // A√∫n falta a√±adir la visualizaci√≥n
  }, CHECK_INTERVAL_RANGE);
}
```
- **Resultado esperado**: El video se controla autom√°ticamente seg√∫n las caras detectadas

---

### Concepto 5: Visualizaci√≥n con Canvas
- **Explicaci√≥n**: Para entender mejor c√≥mo funciona la detecci√≥n, dibujamos rect√°ngulos alrededor de las caras detectadas usando Canvas.
- **Demo RunJS**:
```js
// Simular el dibujo de detecciones
function simularVisualizacion() {
  const carasDetectadas = [
    { topLeft: [50, 60], bottomRight: [150, 180] }
  ];
  
  console.log('üé® Dibujando visualizaci√≥n:');
  
  if (carasDetectadas.length === 0) {
    console.log('üî¥ Overlay rojo: sin caras detectadas');
    return;
  }
  
  carasDetectadas.forEach((cara, index) => {
    const ancho = cara.bottomRight[0] - cara.topLeft[0];
    const alto = cara.bottomRight[1] - cara.topLeft[1];
    
    console.log(`üü¢ Rect√°ngulo ${index + 1}: ${ancho}x${alto}`);
  });
}

simularVisualizacion();
```
- **Aplicaci√≥n a la fase**: Creamos `visualizeFaces()` y completamos el sistema de detecci√≥n
```js
function visualizeFaces(faces) {
  const context = canvas.getContext('2d');

  context.clearRect(0, 0, canvas.width, canvas.height);

  context.drawImage(webcamVideo, 0, 0);

  if (faces.length === 0) { // Si el array est√° vac√≠o
    context.fillStyle = 'rgba(255,0,0,0.3)';
    context.fillRect(0, 0, canvas.width, canvas.height);
    return;
  }

  // Recorrer cada elemento del array (como hicimos en el demo)
  faces.forEach((face) => {
    context.strokeStyle = 'green';
    context.lineWidth = 2;
    context.strokeRect(
      face.topLeft[0],
      face.topLeft[1],
      face.bottomRight[0] - face.topLeft[0],
      face.bottomRight[1] - face.topLeft[1]
    );
  });
}

// ¬°Completamos startDetecting() con todas las funciones!
async function startDetecting() {
  await initCamera();
  model = await loadModel();
  
  checkInterval = setInterval(async () => {
    const faces = await detectFaces();         // Detectar caras
    visualizeFaces(faces);                     // Dibujar detecci√≥n ¬°NUEVO!
    handleAttentionState(faces.length > 0);    // Controlar video
  }, CHECK_INTERVAL_RANGE);
}
```
- **Resultado esperado**: Vemos rect√°ngulos verdes alrededor de las caras + sistema completo

---

### Concepto 6: Integraci√≥n final del sistema
- **Explicaci√≥n**: Actualizamos la funci√≥n `init()` para usar todas las funciones que hemos creado y completar el sistema.
- **Demo RunJS**:
```js
// Simular la integraci√≥n final
let systemRunning = false;

async function finalSystemToggle() {
  try {
    if (systemRunning) {
      console.log('üõë Deteniendo sistema completo...');
      // stopDetecting();
      systemRunning = false;
    } else {
      console.log('üöÄ Iniciando sistema completo...');
      // await startDetecting();
      systemRunning = true;
    }
    
    console.log(`Estado final: ${systemRunning ? 'FUNCIONANDO' : 'DETENIDO'}`);
  } catch (error) {
    console.log('‚ùå Error:', error);
    systemRunning = false;
  }
}

await finalSystemToggle();
```
- **Aplicaci√≥n a la fase**: Actualizamos `init()` para usar el sistema completo de detecci√≥n
```js
async function init() {
  try {
    if (isDetecting) {
      stopDetecting();      // Usar funci√≥n que ya creamos
      isDetecting = false;
    } else {
      await startDetecting();  // Usar funci√≥n completa que ya creamos
      isDetecting = true;
    }

    startStopButton.querySelector('span').textContent = isDetecting
      ? 'Parar'
      : 'Iniciar';
  } catch (error) {
    isDetecting = false;
    console.warn('Hey we fucked up ', error);
  }
}

// El event listener sigue igual
startStopButton.addEventListener('click', init);
```
- **Resultado esperado**: El sistema completo funciona con un solo click del bot√≥n

---

## C√≥digo completo de la fase (SISTEMA FINAL)
```js
// ===== VARIABLES GLOBALES =====
const videoPlayer = document.querySelector('#video-player');
const canvas = document.querySelector('#canvas');
const webcamVideo = document.querySelector('#webcam');
const startStopButton = document.querySelector('#start-stop');
const attentionStatusElement = document.querySelector('#attention-status');

const CHECK_INTERVAL_RANGE = 2000;

let isDetecting = false;
let model = null;
let checkInterval;

// ===== FUNCIONES DEL SISTEMA =====
async function initCamera() {
  const constraints = {
    video: {
      width: 320,
      height: 240,
      facingMode: 'user',
    },
    audio: false,
  };

  const stream = await navigator.mediaDevices.getUserMedia(constraints);

  webcamVideo.srcObject = stream;

  webcamVideo.onloadedmetadata = (metadata) => {
    canvas.width = webcamVideo.videoWidth;
    canvas.height = webcamVideo.videoHeight;
  };

  return true;
}

async function loadModel() {
  const model = await blazeface.load();
  return model;
}

async function detectFaces() {
  if (!model) throw TypeError('Model not loaded');

  const predictions = await model.estimateFaces(webcamVideo, false);

  return predictions;
}

function handleAttentionState(areFacesDetected) {
  if (areFacesDetected) {
    attentionStatusElement.textContent = '‚úÖ Atento';
    attentionStatusElement.className = 'paying-attention';
    document.body.classList.remove('not-paying-attention');

    return videoPlayer.play();
  }

  attentionStatusElement.textContent = '‚ùå No atento';
  attentionStatusElement.className = 'not-paying-attention';
  document.body.classList.add('not-paying-attention');

  videoPlayer.pause();
}

function visualizeFaces(faces) {
  const context = canvas.getContext('2d');

  context.clearRect(0, 0, canvas.width, canvas.height);

  context.drawImage(webcamVideo, 0, 0);

  if (faces.length === 0) {
    context.fillStyle = 'rgba(255,0,0,0.3)';
    context.fillRect(0, 0, canvas.width, canvas.height);
    return;
  }

  faces.forEach((face) => {
    context.strokeStyle = 'green';
    context.lineWidth = 2;
    context.strokeRect(
      face.topLeft[0],
      face.topLeft[1],
      face.bottomRight[0] - face.topLeft[0],
      face.bottomRight[1] - face.topLeft[1]
    );
  });
}

async function startDetecting() {
  await initCamera();

  model = await loadModel();

  checkInterval = setInterval(async () => {
    const faces = await detectFaces();      // Array de caras detectadas (Concepto 3)

    visualizeFaces(faces);                  // Dibujar cada cara del array (Concepto 5)

    handleAttentionState(faces.length > 0); // ¬øEl array tiene elementos? (Concepto 4)
  }, CHECK_INTERVAL_RANGE);
}

function stopDetecting() {
  clearInterval(checkInterval);
}

// ===== FUNCI√ìN PRINCIPAL FINAL =====
async function init() {
  try {
    if (isDetecting) {
      stopDetecting();

      isDetecting = false;
    } else {
      await startDetecting();

      isDetecting = true;
    }

    startStopButton.querySelector('span').textContent = isDetecting
      ? 'Parar'
      : 'Iniciar';
  } catch (error) {
    isDetecting = false;
    console.warn('Hey we fucked up ', error);
  }
}

// ===== INICIALIZACI√ìN =====
startStopButton.addEventListener('click', init);
```

## Ejercicios Interactivos (5 min)

### Ejercicio 1: Arrays y detecci√≥n facial
```js
// Simular resultado de detectFaces()
const resultadoDeteccion = [
  { topLeft: [10, 20], bottomRight: [100, 120] },
  { topLeft: [200, 50], bottomRight: [280, 130] }
];

console.log('N√∫mero de caras:', resultadoDeteccion.length);
console.log('¬øHay caras?', resultadoDeteccion.length > 0);
console.log('Primera cara:', resultadoDeteccion[0].topLeft);
```
**Pregunta:** ¬øQu√© tres valores aparecer√°n?
A) `2`, `true`, `[10, 20]`  B) `1`, `false`, `undefined`  C) `2`, `true`, `10`

---

### Ejercicio 2: setInterval y detecci√≥n continua
```js
let contador = 0;

const intervalo = setInterval(() => {
  contador++;
  console.log('Verificaci√≥n:', contador);
  
  if (contador >= 3) {
    clearInterval(intervalo);
    console.log('Detecci√≥n detenida');
  }
}, 1000);
```
**Pregunta:** ¬øCu√°ntas l√≠neas veremos y en cu√°nto tiempo?
A) 3 l√≠neas en 3 segundos  B) 4 l√≠neas en 3 segundos  C) 4 l√≠neas en 4 segundos

---

### Ejercicio 3: Sistema completo
```js
async function sistemaCompleto() {
  try {
    console.log('Iniciando...');
    
    const caras = []; // Simular sin caras detectadas
    
    if (caras.length > 0) {
      console.log('‚úÖ Usuario atento');
      return 'video_play';
    } else {
      console.log('‚ùå Usuario no atento');  
      return 'video_pause';
    }
  } catch (error) {
    console.log('Error:', error);
    return 'error';
  }
}

sistemaCompleto().then(resultado => console.log('Acci√≥n:', resultado));
```
**Pregunta:** ¬øQu√© tres l√≠neas aparecer√°n?
A) `"Iniciando..."`, `"‚úÖ Usuario atento"`, `"Acci√≥n: video_play"`  B) `"Iniciando..."`, `"‚ùå Usuario no atento"`, `"Acci√≥n: video_pause"`  C) Solo `"Error:"` y `"Acci√≥n: error"`

---

## Troubleshooting r√°pido
- **"Model not loaded"** ‚Üí TensorFlow.js no est√° incluido o hay problemas de conexi√≥n
- **Detecci√≥n muy lenta** ‚Üí Es normal, el modelo tarda en procesar cada frame
- **Muchos falsos negativos** ‚Üí Mejorar iluminaci√≥n, asegurar que la cara sea visible
- **Canvas no muestra video** ‚Üí Verificar que initCamera() configur√≥ las dimensiones
- **Error "Hey we fucked up"** ‚Üí Revisar consola para error espec√≠fico

## Checkpoint: ¬øQu√© deber√≠amos tener funcionando?
- [ ] Al hacer click en "Iniciar", se carga el modelo de IA
- [ ] La detecci√≥n facial funciona autom√°ticamente cada 2 segundos
- [ ] El video se pausa cuando no detecta caras
- [ ] El video se reproduce cuando detecta caras  
- [ ] El estado "‚úÖ Atento" / "‚ùå No atento" se actualiza en tiempo real
- [ ] Se ven rect√°ngulos verdes alrededor de las caras detectadas
- [ ] El sistema completo funciona de principio a fin sin errores
- [ ] Resolvimos correctamente los 3 ejercicios interactivos
- [ ] **TODO el c√≥digo original est√° implementado y funcional**